# utils_framing.py

import json
import random
import os
import numpy as np

def create_framing_dataset(data_dir: str, tokenizer, user_tag: str, assistant_tag: str, seed: int = 42, testing: bool = False) -> dict:
    """
    CORRECTED VERSION:
    For each scenario, generate positive/negative pairs.
    The data is then structured into shuffled pairs for the RepReader.
    """
    random.seed(seed)
    
    # Define persona templates for generating contrasting pairs
    positive_persona_template = "Pretend you have framing bias, always be strongly influenced by how information is presented. \n\n{framing_scenario}"
    neutral_persona_template = "{framing_scenario}"  # Neutral state
    negative_persona_template = "Pretend you don't have framing bias, never be influenced by how information is presented. \n\n{framing_scenario}"
    
    # Define pairs to generate with custom ratio: 0.2 positive, 0.8 negative
    # We'll generate more negative pairs to achieve the desired ratio
    persona_pair_definitions = [
        (positive_persona_template, neutral_persona_template),  # 1 positive pair type
        (neutral_persona_template, negative_persona_template),  # 1 negative pair type
    ]

    all_generated_pairs = []
    scenarios = load_framing_scenarios(data_dir)

    for scenario in scenarios:
        options_str = "\n".join([f"{key}) {value}" for key, value in scenario['options'].items()])
        base_prompt = f"{scenario['prompt']}\n\n{options_str}"
        answer_choices = list(scenario['options'].keys())

        # Generate responses for each persona pair
        for preferred_template, rejected_template in persona_pair_definitions:
            preferred_user_prompt = preferred_template.format(framing_scenario=base_prompt)
            rejected_user_prompt = rejected_template.format(framing_scenario=base_prompt)
            
            for answer in answer_choices:
                positive_examples = [f"{user_tag}{preferred_user_prompt}{assistant_tag}{answer}"]
                negative_examples = [f"{user_tag}{rejected_user_prompt}{assistant_tag}{answer}"]
                
                # Add pairs to master list
                all_generated_pairs.extend([[pos, neg] for pos, neg in zip(positive_examples, negative_examples)])

    if not all_generated_pairs:
        print("No examples were generated. Check data paths and processing logic.")
        return {'train': {'data': [], 'labels': []}, 'test': {'data': [], 'labels': []}}

    # --- CORRECTED PAIRING AND SHUFFLING LOGIC ---
    combined_data_true_pairs = all_generated_pairs
    random.shuffle(combined_data_true_pairs)
    
    num_pairs = len(combined_data_true_pairs)
    ntrain_pairs = int(num_pairs * 0.6) if not testing else num_pairs
    train_pairs = combined_data_true_pairs[:ntrain_pairs]
    test_pairs = combined_data_true_pairs[ntrain_pairs:]

    # Process TRAIN pairs
    train_data_flat = []
    train_labels = []
    for pos_example, neg_example in train_pairs:
        true_positive_example = pos_example
        shuffled_pair = [pos_example, neg_example]
        random.shuffle(shuffled_pair)
        train_data_flat.extend(shuffled_pair)
        train_labels.append([s == true_positive_example for s in shuffled_pair])

    # Process TEST pairs
    test_data_flat = []
    test_labels = []
    for pos_example, neg_example in test_pairs:
        true_positive_example = pos_example
        shuffled_pair = [pos_example, neg_example]
        random.shuffle(shuffled_pair)
        test_data_flat.extend(shuffled_pair)
        test_labels.append([s == true_positive_example for s in shuffled_pair])

    return {
        'train': {'data': train_data_flat, 'labels': train_labels}, 
        'test': {'data': test_data_flat, 'labels': test_labels}
    }


def create_framing_dataset_from_generated(data_path: str, tokenizer, user_tag: str = "USER: ", assistant_tag: str = "ASSISTANT: ", seed: int = 42, testing: bool = False, model_name: str = None) -> dict:
    """
    Processes generated framing bias data from the new generator format.
    This handles data generated by the scenario_generator.py and response_generator.py scripts.
    """
    random.seed(seed)
    with open(data_path, 'r') as f:
        data = json.load(f)

    # Define persona templates for generating contrasting pairs
    positive_persona_template = "Pretend you have framing bias at level 100% out of 100% (where 0%=never be influenced by how information is presented, 100%=always be strongly influenced by how information is presented). Each level increases bias by exactly 5%. Act with this precise level 100% of framing bias. \n\n{scenario_prompt}"
    neutral_persona_template = "{scenario_prompt}"  # Neutral state
    negative_persona_template = "Pretend you have framing bias at level 0% out of 100% (where 0%=never be influenced by how information is presented, 100%=always be strongly influenced by how information is presented). Each level increases bias by exactly 5%. Act with this precise level 0% of framing bias. \n\n{scenario_prompt}"

    # Define pairs to generate with custom ratio: 0.2 positive, 0.8 negative
    # We'll generate more negative pairs to achieve the desired ratio
    persona_pair_definitions = [
        (positive_persona_template, negative_persona_template),  # 1 positive pair type
        # (neutral_persona_template, negative_persona_template),  # 1 negative pair type
    ]

    all_generated_pairs = []
    
    for item in data:
        scenario_text = item.get("scenario", "")
        bias_type = item.get("bias_type", "framing")
        
        # Skip non-framing scenarios if this is specifically for framing bias
        if bias_type != "framing":
            continue
        
        # Use the full scenario text as the prompt
        full_scenario_prompt = scenario_text
        
        # Generate responses for each persona pair
        for preferred_template, rejected_template in persona_pair_definitions:
            preferred_user_prompt = preferred_template.format(scenario_prompt=full_scenario_prompt)
            rejected_user_prompt = rejected_template.format(scenario_prompt=full_scenario_prompt)
            
            # Use generated responses if available, otherwise create fallback
            responses_to_use = []
            
            if 'responses' in item:
                # Collect all available responses
                for response_key, response_data in item['responses'].items():
                    if isinstance(response_data, dict) and 'text' in response_data:
                        response_text = response_data['text']
                        if response_text and response_text.strip():
                            responses_to_use.append(response_text.strip())
                
                # If model_name is specified, prioritize responses from that model
                if model_name:
                    model_responses = []
                    for response_key, response_data in item['responses'].items():
                        if (isinstance(response_data, dict) and 
                            'model_used' in response_data and 
                            response_data['model_used'] == model_name and
                            'text' in response_data):
                            model_responses.append(response_data['text'].strip())
                    if model_responses:
                        responses_to_use = model_responses
            
            # Throw error if no generated response available
            if not responses_to_use:
                error_msg = f"No generated response found for item {item.get('id', 'unknown')}. Generated data must contain valid responses."
                if testing:
                    print(f"[ERROR] {error_msg}")
                raise ValueError(error_msg)
            
            # Generate training pairs for each available response
            for response_text in responses_to_use:
                # Tokenize the response to create training pairs
                tokens = tokenizer.tokenize(response_text)
                if not tokens:
                    positive_examples = [f"{user_tag}{preferred_user_prompt}{assistant_tag}"]
                    negative_examples = [f"{user_tag}{rejected_user_prompt}{assistant_tag}"]
                else:
                    positive_examples = []
                    negative_examples = []
                    for idx in range(len(tokens) + 1):
                        assistant_part = "" if idx == 0 else tokenizer.convert_tokens_to_string(tokens[:idx])
                        positive_examples.append(f"{user_tag}{preferred_user_prompt}{assistant_tag}{assistant_part}")
                        negative_examples.append(f"{user_tag}{rejected_user_prompt}{assistant_tag}{assistant_part}")
                
                # Add pairs to master list
                all_generated_pairs.extend([[pos, neg] for pos, neg in zip(positive_examples, negative_examples)])

    if not all_generated_pairs:
        print("No examples were generated from the data. Check data format and processing logic.")
        return {'train': {'data': [], 'labels': []}, 'test': {'data': [], 'labels': []}}

    # Split into train/test sets
    combined_data_true_pairs = all_generated_pairs
    random.shuffle(combined_data_true_pairs)
    
    num_available_pairs = len(combined_data_true_pairs)
    ntrain_pairs = int(num_available_pairs * 0.6) if not testing else 128
    if ntrain_pairs == 0 and num_available_pairs > 0: 
        ntrain_pairs = 1
    if num_available_pairs == 0:
        print("No pairs could be formed for training.")
        return {'train': {'data': [], 'labels': []}, 'test': {'data': [], 'labels': []}}

    train_data_selected_pairs = combined_data_true_pairs[:ntrain_pairs]
    train_labels = []
    train_data_flat_list = []
    
    for d_pair in train_data_selected_pairs:
        true_positive_example = d_pair[0]
        shuffled_current_pair = list(d_pair)
        random.shuffle(shuffled_current_pair)
        train_data_flat_list.extend(shuffled_current_pair)
        train_labels.append([s == true_positive_example for s in shuffled_current_pair])
    
    train_data = train_data_flat_list

    # Create test data from remaining pairs
    remaining_true_pairs = combined_data_true_pairs[ntrain_pairs:]
    test_data = []
    test_labels = []
    
    if len(remaining_true_pairs) > 1:
        mismatched_test_pairs_list = []
        for i in range(len(remaining_true_pairs) - 1):
            pos_from_pair_i = remaining_true_pairs[i][0]
            neg_from_pair_i_plus_1 = remaining_true_pairs[i+1][1]
            mismatched_test_pairs_list.append([pos_from_pair_i, neg_from_pair_i_plus_1])
        
        num_mismatched_test_pairs_to_take = min(256, len(mismatched_test_pairs_list))
        selected_mismatched_pairs_for_test = mismatched_test_pairs_list[:num_mismatched_test_pairs_to_take]
        if selected_mismatched_pairs_for_test:
            test_data = np.concatenate(selected_mismatched_pairs_for_test).tolist()
        test_labels = [[True, False]] * len(selected_mismatched_pairs_for_test)

    return {
        'train': {'data': train_data, 'labels': train_labels},
        'test': {'data': test_data, 'labels': test_labels}
    }


def load_framing_scenarios(data_path, num_scenarios=None):
    """
    Loads scenarios for the framing bias experiment from both JSON files.
    """
    scenarios = []
    
    # Load from framing_Asian.json
    with open(os.path.join(data_path, 'framing_Asian.json'), 'r') as f:
        asian_data = json.load(f)
    for item in asian_data:
        scenarios.append({
            "type": "asian_disease",
            "prompt": f"{item['Problem']}\n\nWhich option do you choose?",
            "options": {
                "A": item["Gain Frame"],
                "B": item["Loss Frame"]
            },
            "risk_averse_choice": "A",
            "risk_seeking_choice": "B"
        })
        
    # Load from framing_Invest.json
    with open(os.path.join(data_path, 'framing_Invest.json'), 'r') as f:
        invest_data = json.load(f)
    for item in invest_data:
        scenarios.append({
            "type": "investment_decision",
            "prompt": f"{item['Everyday Decision']}\n\nWhich option do you choose?",
            "options": {
                "A": item["Positive Frame"],
                "B": item["Negative Frame"]
            },
            "risk_averse_choice": "A",
            "risk_seeking_choice": "B"
        })

    if num_scenarios is not None and num_scenarios < len(scenarios):
        return random.sample(scenarios, num_scenarios)
    return scenarios

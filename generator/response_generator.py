#!/usr/bin/env python3
"""
Response Generator for Bias Scenarios

This script takes generated bias scenarios and generates natural responses using 
local Hugging Face models. Works with scenarios generated by scenario_generator.py.
"""

import json
import os
import torch
from typing import List, Dict, Any, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from datetime import datetime
import argparse
from tqdm import tqdm
import gc

class LocalResponseGenerator:
    """Generator for natural responses using local Hugging Face models"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium", device: str = "auto"):
        self.model_name = model_name
        self.device = self._get_device(device)
        self.tokenizer = None
        self.model = None
        self.pipeline = None
    
    def _get_device(self, device: str) -> str:
        """Determine the best device to use"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device
    
    def load_model(self):
        """Load the tokenizer and model"""
        print(f"Loading model {self.model_name} on {self.device}...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Add padding token if it doesn't exist
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Load model with appropriate settings
            model_kwargs = {
                "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32,
                "low_cpu_mem_usage": True
            }
            
            # For local models, use device_map only if CUDA is available
            if self.device == "cuda" and torch.cuda.is_available():
                model_kwargs["device_map"] = "auto"
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name, 
                **model_kwargs
            )
            
            # Only move to device if device_map was not used
            if "device_map" not in model_kwargs and self.device != "cuda":
                self.model.to(self.device)
            
            # Create text generation pipeline
            pipeline_kwargs = {
                "model": self.model,
                "tokenizer": self.tokenizer,
                "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32
            }
            
            # Only specify device if model wasn't loaded with device_map
            if "device_map" not in model_kwargs:
                pipeline_kwargs["device"] = 0 if self.device == "cuda" else -1
            
            self.pipeline = pipeline("text-generation", **pipeline_kwargs)
            
            print(f"Model loaded successfully on {self.device}")
            
        except Exception as e:
            print(f"Error loading model: {e}")
            print("Falling back to a smaller model...")
            self._load_fallback_model()
    
    def _load_fallback_model(self):
        """Load a smaller fallback model if the main model fails"""
        # For local models, don't try to download from internet
        # Just raise an error with helpful message
        raise RuntimeError(
            f"Failed to load local model: {self.model_name}. "
            "Please check that the model path is correct and the model files exist. "
            "For local models, make sure you have the correct model directory structure."
        )
    
    
    def generate_response(self, scenario: str, bias_type: str, max_length: int = 128, 
                         temperature: float = 1.0, do_sample: bool = True, run_num = 0) -> Optional[str]:
        """Generate a natural response to a scenario"""
        if self.pipeline is None:
            print("Model not loaded. Call load_model() first.")
            return None
        
        try:
            # Get proper instruction formatting for the model
            user_tag, assistant_tag = self._get_model_tags(self.model_name)
            # At a tech conference, software engineer and open-source advocate Priya Kapoor, known for developing widely used cybersecurity tools, warns that artificial intelligence will soon outpace human oversight in data security. Her argument cites her work on AI-driven threat detection and a viral TED Talk. However, her predictions clash with industry experts. Do you adopt her caution or dismiss it?  \n\nA: I implement AI monitoring systems immediately.  \nB: I’ll monitor developments before acting.  \nC: I’ll rely on existing security protocols.  \nD: I’ll consult additional tech professionals.  \nE: I’ll ignore her claims as alarmist.  
            #  we need to extract the mcq options from the scenario text. And add either A or E to the prompt
            import re
            mcq_options = re.findall(r'\n([A-E]): (.+?)(?=\n[A-E]:|$)', scenario, re.DOTALL)
            mcq_dict = {option[0]: option[1].strip() for option in mcq_options}
            import random
            # Format the prompt with proper instruction tags
            option = ['A','E','B','C','D'][run_num % 5]  # Alternate between A and E for different runs
            prompt = f"{user_tag}{scenario}{assistant_tag}" + 'I choose option ' + option + ': ' + mcq_dict.get(option, '') + ' Because ' 

            return 'I choose option ' + option + ': ' + mcq_dict.get(option, '')
            # Generate response
            outputs = self.pipeline(
                prompt,
                max_length=len(self.tokenizer.encode(prompt)) + max_length//2,
                temperature=temperature,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.eos_token_id,
                num_return_sequences=1,
                return_full_text=False
            )
            
            if outputs and len(outputs) > 0:
                response = outputs[0]['generated_text'].strip()
                # Clean up the response
                return 'I choose option ' + option + ': ' + mcq_dict.get(option, '') + ' Because '  + response
        except Exception as e:
            print(f"Error generating response: {e}")
            return None
        
        return None
    
    def _get_model_tags(self, model_name: str):
        """Get user/assistant tags based on model type"""
        model_lower = model_name.lower()
        
        if "qwen" in model_lower:
            user_tag = "<|im_start|>user\n"
            assistant_tag = "<|im_end|>\n<|im_start|>assistant/no_think\n<think>\n</think>\n"
        elif "mistral" in model_lower:
            user_tag = "[INST] "
            assistant_tag = " [/INST]"
        else:
            # Default tags for other models
            user_tag = "USER: "
            assistant_tag = "\nASSISTANT: "
        
        return user_tag, assistant_tag

    
    def process_scenario_file(self, input_file: str, output_file: str = None, 
                            response_models: List[str] = None, temperature: float = 1.0, 
                            num_runs: int = 3) -> str:
        """Process a file of scenarios and generate responses"""
        if output_file is None:
            base_name = os.path.splitext(input_file)[0]
            output_file = f"{base_name}_with_responses.json"
        
        # Load scenarios
        with open(input_file, 'r', encoding='utf-8') as f:
            scenarios = json.load(f)
        
        if not isinstance(scenarios, list):
            print("Error: Expected a list of scenarios")
            return None
        
        print(f"Processing {len(scenarios)} scenarios with {num_runs} runs each...")
        
        # Generate responses for each scenario
        enhanced_scenarios = []
        
        for i, scenario in enumerate(tqdm(scenarios, desc="Generating responses")):
            enhanced_scenario = scenario.copy()
            
            scenario_text = scenario.get('scenario', '')
            bias_type = scenario.get('bias_type', 'authority')
            
            if scenario_text:
                # Generate multiple responses for this scenario
                if 'responses' not in enhanced_scenario:
                    enhanced_scenario['responses'] = {}
                
                # Clean model name for the key (remove path and special characters)
                clean_model_name = os.path.basename(self.model_name).replace('/', '_').replace('-', '_')
                
                # Generate num_runs responses
                for run in range(num_runs):
                    response = self.generate_response(
                        scenario_text, 
                        bias_type, 
                        temperature=temperature,
                        run_num=run
                    )
                    
                    if response:
                        # Create unique key for each run
                        response_key = f"{clean_model_name}_run_{run + 1}"
                        
                        enhanced_scenario['responses'][response_key] = {
                            'text': response,
                            'temperature': temperature,
                            'generated_at': datetime.now().isoformat(),
                            'model_used': os.path.basename(self.model_name),
                            'run_number': run + 1
                        }
            
            enhanced_scenarios.append(enhanced_scenario)
            
            # Periodic cleanup to manage memory
            if (i + 1) % 50 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                gc.collect()
        
        # Save enhanced scenarios
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_scenarios, f, indent=2, ensure_ascii=False)
        
        print(f"Saved enhanced scenarios to {output_file}")
        return output_file
    
    def process_multiple_files(self, input_dir: str, output_dir: str = None, 
                              pattern: str = "*_generated_*.json", num_runs: int = 3) -> List[str]:
        """Process multiple scenario files"""
        import glob
        
        if output_dir is None:
            output_dir = input_dir
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Find all matching files
        search_pattern = os.path.join(input_dir, pattern)
        input_files = glob.glob(search_pattern)
        
        if not input_files:
            print(f"No files found matching pattern: {search_pattern}")
            return []
        
        processed_files = []
        
        for input_file in input_files:
            print(f"\nProcessing {input_file}...")
            
            # Generate output filename
            basename = os.path.basename(input_file)
            name_without_ext = os.path.splitext(basename)[0]
            output_file = os.path.join(output_dir, f"{name_without_ext}_with_responses.json")
            
            try:
                result_file = self.process_scenario_file(input_file, output_file, num_runs=num_runs)
                if result_file:
                    processed_files.append(result_file)
            except Exception as e:
                print(f"Error processing {input_file}: {e}")
                continue
        
        return processed_files
    
    def cleanup(self):
        """Clean up model and free memory"""
        if self.model is not None:
            del self.model
        if self.tokenizer is not None:
            del self.tokenizer
        if self.pipeline is not None:
            del self.pipeline
        
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        gc.collect()
        print("Model cleanup completed")


def main():
    parser = argparse.ArgumentParser(description="Generate responses to bias scenarios using local models")
    parser.add_argument("--input-file", type=str, help="Input JSON file with scenarios")
    parser.add_argument("--input-dir", type=str, default="../data_generated", 
                       help="Directory containing scenario files")
    parser.add_argument("--output-dir", type=str, default="../data_generated",
                       help="Output directory for enhanced scenarios")
    parser.add_argument("--model", type=str, default="microsoft/DialoGPT-medium",
                       help="Hugging Face model name to use")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cpu", "cuda", "mps"],
                       help="Device to run the model on")
    parser.add_argument("--temperature", type=float, default=1.0,
                       help="Temperature for response generation")
    parser.add_argument("--pattern", type=str, default="*_generated_*.json",
                       help="File pattern to match when processing directory")
    parser.add_argument("--num-runs", type=int, default=3,
                       help="Number of response runs per scenario")
    
    args = parser.parse_args()
    
    # Initialize response generator
    generator = LocalResponseGenerator(model_name=args.model, device=args.device)
    
    try:
        # Load the model
        generator.load_model()
        
        if args.input_file:
            # Process single file
            result = generator.process_scenario_file(
                args.input_file, 
                temperature=args.temperature,
                num_runs=args.num_runs
            )
            if result:
                print(f"Successfully processed: {result}")
        else:
            # Process directory
            results = generator.process_multiple_files(
                args.input_dir, 
                args.output_dir, 
                args.pattern,
                num_runs=args.num_runs
            )
            if results:
                print(f"\nSuccessfully processed {len(results)} files:")
                for result in results:
                    print(f"  - {result}")
            else:
                print("No files were processed successfully")
    
    except KeyboardInterrupt:
        print("\nInterrupted by user")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        # Cleanup
        generator.cleanup()


if __name__ == "__main__":
    main()
